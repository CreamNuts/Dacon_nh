{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('nh': conda)",
   "metadata": {
    "interpreter": {
     "hash": "2045e08fbc2959d0b9d7a07b2eadae18ae768b41abcfd08a9293b8d2f340997e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('./data/news_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 118745/118745 [01:54<00:00, 1041.09it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import tqdm\n",
    "from khaiii import KhaiiiApi\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "def text_preprocessing(text_list):\n",
    "    \n",
    "    stopwords = ['을', '를', '이', '가', '은', '는', 'null']\n",
    "    tokenizer = KhaiiiApi()\n",
    "    token_list = []\n",
    "\n",
    "    for text in tqdm.tqdm(text_list):\n",
    "        txt = re.sub('[^가-힣a-z]', ' ', text.lower()).strip()\n",
    "        if not txt: txt = 'null'\n",
    "        morphs = []\n",
    "        for word in tokenizer.analyze(txt):\n",
    "            for morph in word.morphs:\n",
    "                morphs.append(morph.lex) #품사는 morph.tag로 표현 가능\n",
    "        token_list.append([t for t in morphs if t not in stopwords or type(t) != float])\n",
    "    return token_list, tokenizer\n",
    "\n",
    "train['token'], tokenizer = text_preprocessing(train['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "vocab_size :  48514\n",
      "(118745, 100) (118745,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def text2sequence(train_text, max_len=1000):\n",
    "    vectorizer = Tokenizer()\n",
    "    vectorizer.fit_on_texts(train_text)\n",
    "    train_X_seq = vectorizer.texts_to_sequences(train_text)\n",
    "    vocab_size = len(vectorizer.word_index) + 1\n",
    "    print('vocab_size : ', vocab_size)\n",
    "    X_train = pad_sequences(train_X_seq, maxlen = max_len)\n",
    "    return X_train, vocab_size, vectorizer\n",
    "\n",
    "train_y = train['info']\n",
    "train_X, vocab_size, vectorizer = text2sequence(train['token'], max_len = 100)\n",
    "print(train_X.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "vocab_size : 48514\nword2vec_vocab_size : (48513, 100)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "# word2vec = gensim.models.Word2Vec.load('ko.bin')\n",
    "#word2vec = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary = True)\n",
    "word2vec = gensim.models.Word2Vec(sentences=train['token'].values, min_count=1, workers=4)\n",
    "# word2vec.build_vocab(list(train['token'].values), update=True)\n",
    "# word2vec.train(list(train['token'].values), total_examples=len(list(train['token'].values)), epochs=10)\n",
    "print(f'vocab_size : {vocab_size}')\n",
    "print(f'word2vec_vocab_size : {word2vec.wv.vectors.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 48513/48513 [00:00<00:00, 82484.22it/s]총 0개의 단어가 word2vec에 없습니다.\n",
      "embedding_matrix shape : (48514, 100)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def embedding(vocabulary):\n",
    "    num = 0\n",
    "    embedding_matrix = np.zeros((vocab_size, 100))\n",
    "    for index, word in enumerate((tqdm.tqdm(vocabulary))):\n",
    "        if word in word2vec:\n",
    "            embedding_vector = word2vec[word] \n",
    "            embedding_matrix[index] = embedding_vector \n",
    "        else:\n",
    "            #print(f\"'{word}'는 word2vec에 없는 단어입니다.\")\n",
    "            num += 1\n",
    "    print(f\"총 {num}개의 단어가 word2vec에 없습니다.\")\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = embedding(vectorizer.word_index)\n",
    "print(f'embedding_matrix shape : {embedding_matrix.shape}') #word2vec.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_2 (Embedding)      (None, 100, 100)          4851400   \n_________________________________________________________________\nspatial_dropout1d_2 (Spatial (None, 100, 100)          0         \n_________________________________________________________________\nlstm_2 (LSTM)                (None, 64)                42240     \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 64)                0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 64)                4160      \n_________________________________________________________________\ndense_5 (Dense)              (None, 1)                 65        \n=================================================================\nTotal params: 4,897,865\nTrainable params: 46,465\nNon-trainable params: 4,851,400\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, regularizers\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dropout,Dense, SpatialDropout1D\n",
    "\n",
    "def model(vocab_size, max_len=100):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, weights = [embedding_matrix], input_length = max_len, trainable=False)) #임베딩 가중치 적용 코드\n",
    "    model.add(SpatialDropout1D(0.3))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer = regularizers.l2(0.001)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = model(len(embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "928/928 [==============================] - 94s 101ms/step - loss: 0.1626 - accuracy: 0.9406\n",
      "Epoch 2/5\n",
      "928/928 [==============================] - 94s 101ms/step - loss: 0.0980 - accuracy: 0.9606\n",
      "Epoch 3/5\n",
      "928/928 [==============================] - 94s 101ms/step - loss: 0.0856 - accuracy: 0.9650\n",
      "Epoch 4/5\n",
      "928/928 [==============================] - 94s 101ms/step - loss: 0.0797 - accuracy: 0.9681\n",
      "Epoch 5/5\n",
      "928/928 [==============================] - 94s 101ms/step - loss: 0.0738 - accuracy: 0.9708\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6e7c367220>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "model.fit(train_X, train_y, batch_size=128, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 142565/142565 [02:17<00:00, 1033.96it/s]\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('./data/news_test.csv')\n",
    "test['token'], tokenizer = text_preprocessing(test['content'])\n",
    "test_X_seq = vectorizer.texts_to_sequences(test['token'])\n",
    "test_X = pad_sequences(test_X_seq, maxlen = 100)\n",
    "result = model.predict(test_X)\n",
    "result = np.where(result>0.5, 1, 0)\n",
    "test['info'] = result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./data/sample_submission.csv')\n",
    "# for idx, id in enumerate(tqdm.tqdm(submission['id'].values)):\n",
    "#     submission['info'].iloc[idx] = test[id == test['id']]['info'].values\n",
    "submission['info'] = test['info']\n",
    "submission.to_csv('simple_baseline_if_in_train_one.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}